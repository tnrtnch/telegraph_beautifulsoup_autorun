ğŸ•·ï¸ Telegraph news with BeautifulSoup
This project downloads current news from telegraph.bg using the beautifulsoup library.
The collected data is stored in a SQLite database (articles.db) and automatically updated every hour via GitHub Actions.

ğŸš€ Features
Extracts article title, body, author, using BeautifulSoup library
Stores all scraped data into a SQLite database (articles.db)
Automated crawling with GitHub Actions (runs every hour) Database updates are version-controlled via Git commits
With each data update, only the last day's data remains in the database file; older data is automatically deleted.


ğŸ“‚Project Structure <br />
telegraph<br />
â”‚<br />
â”œâ”€â”€ .github/workflows/             # GitHub Actions workflows<br />
â”‚&nbsp; &nbsp;  â””â”€â”€actions.yml<br />
â”œâ”€â”€  .gitignore <br />   
â”œâ”€â”€  articles.db <br /> 
â”œâ”€â”€  flowchart_telegraph_latest_news.jpg <br /> 
â”œâ”€â”€  items.py <br /> 
â”œâ”€â”€  main.py <br /> 
â”œâ”€â”€  pipelines.py <br /> 
â”œâ”€â”€  scraper.py <br /> 
â”œâ”€â”€  requirements.txt <br /> 
â””â”€â”€  README.md <br /> 
<br />


â°GitHub Actions (Automated Runs)  <br />
<br />
This project includes a workflow at .github/workflows/actions.yml.<br />
The workflow:<br />
<br />
Runs every hour (via cron job)<br />
Executes the spider<br />
Deletes data older than one day
Updates articles.db with new data<br />
Commits changes back to the repository<br />
You can view run logs under the Actions tab in the repository.<br />
<br />

